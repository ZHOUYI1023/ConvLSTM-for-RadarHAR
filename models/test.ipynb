{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from thop import profile\n",
    "from thop import clever_format\n",
    "import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 128\n",
    "channel_d = 64\n",
    "channel_in = channel_d\n",
    "channel_out = channel_d\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BHW\n",
    "weight = torch.rand(channel_out, channel_in, 3)\n",
    "out =  F.conv1d(f_dt, weight, bias=None, stride=1, padding=1, dilation=1, groups=1)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 3, 3)\n",
    "filters = torch.randn(3, 3, 2)\n",
    "out = F.conv1d(inputs, filters)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4881, -0.2732],\n",
       "         [-1.0998,  0.6145],\n",
       "         [-0.3854, -0.4885]],\n",
       "\n",
       "        [[-1.0726, -0.7862],\n",
       "         [ 0.2072, -0.1803],\n",
       "         [ 2.6887,  1.0425]],\n",
       "\n",
       "        [[ 1.8110,  0.2561],\n",
       "         [-1.2803,  0.7140],\n",
       "         [ 0.6212, -0.1925]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9133)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[0]*inputs[0,:,:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6544)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[0]*inputs[0,:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9133,  0.6544],\n",
       "         [-5.5761,  0.7350],\n",
       "         [-1.8866,  1.3048]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "h = channel_d\n",
    "channel_in = channel_d\n",
    "channel_out = channel_d # number of kernels\n",
    "weight = torch.rand(channel_out, np.int(channel_in/h), 3)\n",
    "out =  F.conv1d(f_dt, weight,padding =1, groups=h)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# depthwise conv: the input channel, filter_channel, groups, and output channels must follows\n",
    "# filter_channels = input channels/groups\n",
    "# output channles =  k * input channels/filter_channels=groups\n",
    "# where k behaves like multi-head\n",
    "inputs = torch.randn(1, 6, 3)\n",
    "filters = torch.randn(6, 2, 2)\n",
    "out = F.conv1d(inputs, filters, groups=3)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1123, -0.0244],\n",
       "         [ 0.6030, -1.5401]],\n",
       "\n",
       "        [[-1.8002,  1.1385],\n",
       "         [ 0.6183,  1.6528]],\n",
       "\n",
       "        [[-2.3613,  0.4606],\n",
       "         [ 1.0304,  0.8362]],\n",
       "\n",
       "        [[ 0.5690, -0.5754],\n",
       "         [-0.3750, -0.7164]],\n",
       "\n",
       "        [[-0.7991,  1.6169],\n",
       "         [ 1.0393,  0.2280]],\n",
       "\n",
       "        [[ 0.1741,  2.0219],\n",
       "         [ 0.4011, -0.2367]]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.5472)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[0]*inputs[0,0:2,:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0578)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[0]*inputs[0,0:2,1:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0647)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[1]*inputs[0,0:2,:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.9396)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[2]*inputs[0,2:4,:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5472,  3.0578],\n",
       "         [ 1.0647, -3.7603],\n",
       "         [-2.9396,  0.3557],\n",
       "         [ 0.7460,  1.1228],\n",
       "         [-1.5871, -1.9176],\n",
       "         [ 0.2089, -2.0115]]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 6, 3)\n",
    "filters = torch.randn(3, 2, 2)\n",
    "out = F.conv1d(inputs, filters, groups=3)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2131, -0.3739],\n",
       "         [ 0.1408, -1.5768]],\n",
       "\n",
       "        [[-0.8509, -0.6289],\n",
       "         [ 0.3753, -0.2520]],\n",
       "\n",
       "        [[-0.2391,  0.0474],\n",
       "         [-1.0111, -2.1080]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 6, 3)\n",
    "inputs = inputs.view(-1, 3, 3)\n",
    "filters = torch.randn(3, 1, 3)\n",
    "out = F.conv1d(inputs, filters, padding=1, groups=3)\n",
    "out = out.view(1, 6, 3)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1740)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[0]*inputs[0,0,:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1060)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[1]*inputs[0,1,:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1466)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[2]*inputs[0,2,:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6573)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[0]*inputs[1,0,:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0046)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[1]*inputs[1,1,:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs =  inputs.view(1, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1740)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[0]*inputs[0,0,:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6573)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(filters[0]*inputs[0,3,:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3233, -0.1740,  0.5302],\n",
       "         [-0.5915, -0.1060,  1.5928],\n",
       "         [-0.2367,  0.1466, -0.0522],\n",
       "         [ 1.4719, -1.6573,  1.6423],\n",
       "         [-0.2396,  0.0046,  0.6375],\n",
       "         [-0.7814,  1.1654, -0.8154]]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test LightweightConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128, 64]) torch.Size([16, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 128\n",
    "channel_d = 64\n",
    "f_td = torch.rand(batch_size, seq_len_t, channel_d) # BWH\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BHW\n",
    "print(f_td.shape, f_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightConv(nn.Module):\n",
    "    '''Lightweight convolution from fairseq.\n",
    "    Args:\n",
    "        input_size: # of channels of the input and output\n",
    "        kernel_size: convolution channels\n",
    "        padding: padding\n",
    "        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)\n",
    "        weight_softmax: normalize the weight with softmax before the convolution\n",
    "        dropout: dropout probability\n",
    "    Forward:\n",
    "        Input: BxCxT, i.e. (batch_size, input_size, timesteps)\n",
    "        Output: BxCxT, i.e. (batch_size, input_size, timesteps)\n",
    "    Attributes:\n",
    "        weight: learnable weights of shape `(num_heads, 1, kernel_size)`\n",
    "        bias:   learnable bias of shape `(input_size)`\n",
    "    '''\n",
    "    def __init__(self, input_size, kernel_size=1, padding=0, n_heads=1,\n",
    "                 weight_softmax=True, bias=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_heads = n_heads\n",
    "        self.padding = padding\n",
    "        self.weight_softmax = weight_softmax\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_heads, 1, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(input_size)) if bias else None\n",
    "        self.dropout = dropout\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, C, T = input.size()\n",
    "        H = self.n_heads\n",
    "        weight = F.softmax(self.weight, dim=-1) if self.weight_softmax else self.weight\n",
    "        weight = F.dropout(weight, self.weight_dropout, training=self.training)\n",
    "        \n",
    "        # Merge every C/H entries into the batch dimension (C = self.input_size)\n",
    "        # B x C x T -> (B * C/H) x H x T\n",
    "        # One can also expand the weight to C x 1 x K by a factor of C/H\n",
    "        # and do not reshape the input instead, which is slow though\n",
    "        input = input.view(-1, H, T)\n",
    "        output = F.conv1d(input, weight, padding=self.padding, groups=H)\n",
    "        output = output.view(B, C, T)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.view(1, -1, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightConv(nn.Module):\n",
    "    def __init__(self, input_size, groups, kernel_size=1, padding=0, n_heads=1,\n",
    "                 weight_softmax=True, bias=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_heads = n_heads\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.weight_softmax = weight_softmax\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_heads*input_size, np.int(input_size/groups), kernel_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(input_size)) if bias else None\n",
    "        self.dropout = dropout\n",
    "        self.weight_dropout = 0.1\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, C, T = input.size()\n",
    "        H = self.n_heads\n",
    "        # weight: n_heads, 1, kernel_size \n",
    "        weight = F.softmax(self.weight, dim=-1) if self.weight_softmax else self.weight\n",
    "        print(weight.shape)\n",
    "        weight = F.dropout(weight, self.weight_dropout, training=self.training)\n",
    "        # Merge every C/H entries into the batch dimension (C = self.input_size)\n",
    "        # B x C x T -> (B * C/H) x H x T\n",
    "        # One can also expand the weight to C x 1 x K by a factor of C/H\n",
    "        # and do not reshape the input instead, which is slow though\n",
    "        #input = input.contiguous().view(-1, H, T)\n",
    "        # input_tensor, kernel, stride=1, padding=1\n",
    "        output = F.conv1d(input, weight, padding=self.padding, groups=self.groups)\n",
    "        #output = output.view(B, C, T)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.view(1, -1, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 1, 3])\n",
      "torch.Size([16, 192, 128])\n"
     ]
    }
   ],
   "source": [
    "model = LightweightConv(input_size=f_dt.shape[1],groups=f_dt.shape[1], kernel_size=3, padding=1, n_heads=3)\n",
    "out = model(f_dt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 1])\n",
      "torch.Size([32, 64, 64])\n",
      "torch.Size([64, 1, 1])\n",
      "torch.Size([32, 64, 64])\n",
      "torch.Size([64, 1, 1])\n",
      "torch.Size([32, 64, 64])\n",
      "torch.Size([64, 1, 1])\n",
      "torch.Size([32, 64, 64])\n",
      "torch.Size([64, 1, 1])\n",
      "torch.Size([32, 64, 64])\n",
      "torch.Size([64, 1, 1])\n",
      "torch.Size([32, 64, 64])\n",
      "Inference Time: 0.0006244182586669922 seconds\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "input_tensor = f_td.to(device)\n",
    "\n",
    "# Warm up the GPU by performing a few inference runs\n",
    "for _ in range(5):\n",
    "    model(input_tensor)\n",
    "\n",
    "# Measure the inference time\n",
    "start_time = time.time()\n",
    "model(input_tensor)\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "print(f\"Inference Time: {inference_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 1])\n",
      "torch.Size([32, 64, 64])\n",
      "0.000B 0.000B\n"
     ]
    }
   ],
   "source": [
    "macs, params = profile(model, inputs=(input_tensor, ))\n",
    "macs, params = clever_format([macs, params], \"%.3f\")\n",
    "print(macs, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SymmetricConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 128\n",
    "channel_d = 64\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BHW\n",
    "print(f_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_half(output):\n",
    "    B, C, T = output.size()\n",
    "    half1 = output[:, :C//2, :]\n",
    "    half2 = output[:, C//2:, :]\n",
    "    half2_flipped = torch.flip(half2, dims=[1])\n",
    "    output = torch.cat((half1, half2_flipped), dim=1)\n",
    "    return output\n",
    "\n",
    "\n",
    "class SymmetricLightweightConv(nn.Module):\n",
    "    def __init__(self, d_size, groups=2, kernel_size=3, padding=1, n_heads=1,\n",
    "                 weight_softmax=True, bias=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = d_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_heads = n_heads\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.weight_softmax = weight_softmax\n",
    "        self.weight = nn.Parameter(torch.Tensor(np.int(n_heads*self.input_size/2), 1, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.input_size)) if bias else None\n",
    "        self.dropout = dropout\n",
    "        self.weight_dropout = 0.1\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, C, T = input.size()\n",
    "        input = flip_half(input)\n",
    "        reshaped_tensor = input.view(B * 2, C // 2, T)\n",
    "        #print(reshaped_tensor.shape)\n",
    "        H = self.n_heads\n",
    "        # weight: n_heads, 1, kernel_size \n",
    "        #normalized_weights = (self.weight - torch.mean(self.weight)) / torch.std(self.weight)\n",
    "        weight = F.softmax(self.weight, dim=-1) if self.weight_softmax else self.weight\n",
    "        weight = F.dropout(weight, self.weight_dropout, training=self.training)\n",
    "        #print(weight.shape)\n",
    "        output = F.conv1d(reshaped_tensor, weight, padding=self.padding, groups=np.int(self.groups/2))\n",
    "        #print(output.shape)\n",
    "        output = output.view(B, C, T)\n",
    "        output = flip_half(output)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.view(1, -1, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "model = SymmetricLightweightConv(d_size=f_dt.shape[1],groups=f_dt.shape[1], kernel_size=3, padding=1, n_heads=1, weight_softmax=False)\n",
    "out = model(f_dt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DynamicConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128, 64]) torch.Size([16, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 128\n",
    "channel_d = 64\n",
    "f_td = torch.rand(batch_size, seq_len_t, channel_d) # BWH\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BHW\n",
    "print(f_td.shape, f_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConv(nn.Module):\n",
    "    def __init__(self, input_size, kernel_size=1, padding=0, n_heads=1,\n",
    "                 weight_softmax=True, bias=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_heads = n_heads\n",
    "        self.padding = padding\n",
    "        self.weight_softmax = weight_softmax\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_heads, 1, kernel_size))\n",
    "        self.weight_linear = nn.Linear(input_size, n_heads * kernel_size, bias)\n",
    "        self.bias = nn.Parameter(torch.Tensor(input_size)) if bias else None\n",
    "        self.dropout = dropout\n",
    "        self.weight_dropout = 0.1\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''Takes input (B x C x T) to output (B x C x T)'''\n",
    "        \n",
    "        # Prepare weight (take softmax)\n",
    "        B, C, T = input.size()\n",
    "        print(B,C,T)\n",
    "        H, K = self.n_heads, self.kernel_size\n",
    "        # weight: n_heads, 1, kernel_size \n",
    "        weight = self.weight_linear(input.permute(0,2,1))\n",
    "        print(weight.shape)\n",
    "        weight = F.softmax(weight, dim=-1) \n",
    "        #weight = F.softmax(self.weight, dim=-1) if self.weight_softmax else self.weight\n",
    "        weight = F.dropout(weight, self.weight_dropout, training=self.training)\n",
    "        weight = weight.permute(0, 2, 3, 1)\n",
    "        weight = weight.contiguous().view(-1, H, T)\n",
    "        input = input.contiguous().view(-1, H, T)\n",
    "        # input_tensor, kernel, stride=1, padding=1\n",
    "        output = F.conv1d(input, weight, padding=self.padding, groups=H)\n",
    "        output = output.view(B, C, T)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 64 128\n",
      "torch.Size([16, 128, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-bab39120ec91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamicConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-222-acbc10c0fc89>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#weight = F.softmax(self.weight, dim=-1) if self.weight_softmax else self.weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "model = DynamicConv(input_size=f_dt.shape[1], kernel_size=1, padding=0, n_heads=f_dt.shape[1])\n",
    "out = model(f_dt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Spectral Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "def _spectral_crop(input, oheight, owidth):\n",
    "    cutoff_freq_h = math.ceil(oheight / 2)\n",
    "    cutoff_freq_w = math.ceil(owidth / 2)\n",
    "\n",
    "    if oheight % 2 == 1:\n",
    "        if owidth % 2 == 1:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            bottom_left = input[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            bottom_left = input[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:]\n",
    "    else:\n",
    "        if owidth % 2 == 1:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            bottom_left = input[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            bottom_left = input[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -cutoff_freq_h:, -cutoff_freq_w:]\n",
    "\n",
    "    top_combined = torch.cat((top_left, top_right), dim=-1)\n",
    "    bottom_combined = torch.cat((bottom_left, bottom_right), dim=-1)\n",
    "    all_together = torch.cat((top_combined, bottom_combined), dim=-2)\n",
    "\n",
    "    return all_together\n",
    "\n",
    "def _spectral_pad(input, output, oheight, owidth):\n",
    "    cutoff_freq_h = math.ceil(oheight / 2)\n",
    "    cutoff_freq_w = math.ceil(owidth / 2)\n",
    "    pad = torch.zeros_like(input)\n",
    "\n",
    "    if oheight % 2 == 1:\n",
    "        if owidth % 2 == 1:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):] = output[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w] = output[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):] = output[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -cutoff_freq_w:] = output[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w] = output[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:] = output[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:]\n",
    "    else:\n",
    "        if owidth % 2 == 1:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):] = output[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            pad[:, :, -cutoff_freq_h:, :cutoff_freq_w] = output[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            pad[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):] = output[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -cutoff_freq_w:] = output[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            pad[:, :, -cutoff_freq_h:, :cutoff_freq_w] = output[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            pad[:, :, -cutoff_freq_h:, -cutoff_freq_w:] = output[:, :, -cutoff_freq_h:, -cutoff_freq_w:]\t\n",
    "\n",
    "    return pad\n",
    "\n",
    "def DiscreteHartleyTransform(input):\n",
    "    fft = torch.rfft(input, 2, normalized=True, onesided=False)\n",
    "    # for new version of pytorch\n",
    "    #fft = torch.fft.fft2(input, dim=(-2, -1), norm='ortho')\n",
    "    #fft = torch.stack((fft.real, fft.imag), -1)\n",
    "    dht = fft[:, :, :, :, -2] - fft[:, :, :, :, -1]\n",
    "    return dht\n",
    "\n",
    "class SpectralPoolingFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, oheight, owidth):\n",
    "        ctx.oh = oheight\n",
    "        ctx.ow = owidth\n",
    "        ctx.save_for_backward(input)\n",
    "\n",
    "        # Hartley transform by RFFT\n",
    "        dht = DiscreteHartleyTransform(input)\n",
    "\n",
    "        # frequency cropping\n",
    "        all_together = _spectral_crop(dht, oheight, owidth)\n",
    "        # inverse Hartley transform\n",
    "        dht = DiscreteHartleyTransform(all_together)\n",
    "        return dht\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_variables\n",
    "\n",
    "        # Hartley transform by RFFT\n",
    "        dht = DiscreteHartleyTransform(grad_output)\n",
    "        # frequency padding\n",
    "        grad_input = _spectral_pad(input, dht, ctx.oh, ctx.ow)\n",
    "        # inverse Hartley transform\n",
    "        grad_input = DiscreteHartleyTransform(grad_input)\n",
    "        return grad_input, None, None\n",
    "\n",
    "class SpectralPool2d(nn.Module):\n",
    "    def __init__(self, t_size):\n",
    "        super(SpectralPool2d, self).__init__()\n",
    "        self.t_size = t_size\n",
    "    def forward(self, input):\n",
    "        H, W = input.size(-2), input.size(-1)\n",
    "        #h, w = math.ceil(H*self.scale_factor[0]), math.ceil(W*self.scale_factor[1])\n",
    "        return SpectralPoolingFunction.apply(input, H, self.t_size)\n",
    "\n",
    "\n",
    "\n",
    "class SpectralPooling_layer(nn.Module):\n",
    "    def __init__(self, t_size):\n",
    "        super(SpectralPooling_layer, self).__init__()\n",
    "        self.t_size = t_size\n",
    "        self.SpecPool2d = SpectralPool2d(t_size=t_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input: batch, in_channel, length \n",
    "        x = x.unsqueeze(1)   # input: batch, 1, in_channel, length \n",
    "        out = self.SpecPool2d(x)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150, 153])\n",
      "torch.Size([16, 150, 128])\n"
     ]
    }
   ],
   "source": [
    "model = SpectralPooling_layer(t_size=128)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "seq_len_t = 153\n",
    "channel_d = 150\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BCT\n",
    "print(f_dt.shape)\n",
    "out =  model(f_dt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Conv1D Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DEncoder(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(Conv1DEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # input: batch, in_channel, length \n",
    "            # conv1d: in_channel, out_channel, kernel, stride, padding\n",
    "            # size: (in_size-kernel+2*padding)/stride + 1\n",
    "            # l_out: batch, out_channel, length\n",
    "\n",
    "            nn.Conv1d(in_channel, 32, 5, 1, 2),  # out: batch * 32 * 112\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2),  # batch * 32 * 56\n",
    "\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(32, 64, 3, 1, 1),  # out: batch * 64 * 56\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(64, 128, 3, 1, 1),  # out: batch * 64 * 56\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x:  batch * C * T\n",
    "        y = self.conv1(x)  # batch * 64 * 112\n",
    "        #print(y.shape)\n",
    "        y = self.conv2(y)\n",
    "        #y = self.conv3(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 150, 153])\n",
      "torch.Size([16, 64, 76])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 153\n",
    "channel_d = 150\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BHW\n",
    "print(f_dt.shape)\n",
    "model = Conv1DEncoder(in_channel = channel_d)\n",
    "out =  model(f_dt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Attenntionl LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention1D(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(Attention1D, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.weight = nn.Linear(in_channel,1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        M = self.tanh(H)  # (batch, seq_len, rnn_size)\n",
    "        alpha = self.weight(M).squeeze(2)  # (batch, seq_len)\n",
    "        alpha = self.softmax(alpha)  # (batch, seq_len)\n",
    "\n",
    "        r = H * alpha.unsqueeze(2) # (batch, seq_len, rnn_size)\n",
    "        r = r.sum(dim=1)  # (batch, rnn_size)\n",
    "\n",
    "        return r, alpha \n",
    "\n",
    "class Attentional_LSTM_Pool(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Attentional_LSTM_Pool,self).__init__()\n",
    "        \n",
    "        self.attention = Attention1D(in_channel=32)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=32, num_layers=3, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(96, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # x:  B * T * C\n",
    "        x1, (ht,ct) = self.lstm(x) # x1: B, T, bi*hidden_size\n",
    "        x1, _ = self.attention(x1) # out: batch, bi*hidden_size: 64\n",
    "        x2 = torch.max(x, 1, keepdim=False)[0] #  B * C\n",
    "        x_all = torch.cat((x1,x2),dim=1)\n",
    "        out = self.fc(x_all)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 76, 64])\n",
      "torch.Size([16, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 76\n",
    "channel_d = 64\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BCT\n",
    "f_td = f_dt.transpose(2,1)\n",
    "print(f_td.shape)\n",
    "model = Attentional_LSTM_Pool(input_size = channel_d, num_classes = 5)\n",
    "out =  model(f_td)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "\n",
    "def _spectral_crop(input, oheight, owidth):\n",
    "    cutoff_freq_h = math.ceil(oheight / 2)\n",
    "    cutoff_freq_w = math.ceil(owidth / 2)\n",
    "\n",
    "    if oheight % 2 == 1:\n",
    "        if owidth % 2 == 1:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            bottom_left = input[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            bottom_left = input[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:]\n",
    "    else:\n",
    "        if owidth % 2 == 1:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            bottom_left = input[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            bottom_left = input[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -cutoff_freq_h:, -cutoff_freq_w:]\n",
    "\n",
    "    top_combined = torch.cat((top_left, top_right), dim=-1)\n",
    "    bottom_combined = torch.cat((bottom_left, bottom_right), dim=-1)\n",
    "    all_together = torch.cat((top_combined, bottom_combined), dim=-2)\n",
    "\n",
    "    return all_together\n",
    "\n",
    "def _spectral_pad(input, output, oheight, owidth):\n",
    "    cutoff_freq_h = math.ceil(oheight / 2)\n",
    "    cutoff_freq_w = math.ceil(owidth / 2)\n",
    "    pad = torch.zeros_like(input)\n",
    "\n",
    "    if oheight % 2 == 1:\n",
    "        if owidth % 2 == 1:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):] = output[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w] = output[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):] = output[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -cutoff_freq_w:] = output[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w] = output[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:] = output[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:]\n",
    "    else:\n",
    "        if owidth % 2 == 1:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):] = output[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            pad[:, :, -cutoff_freq_h:, :cutoff_freq_w] = output[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            pad[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):] = output[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -cutoff_freq_w:] = output[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            pad[:, :, -cutoff_freq_h:, :cutoff_freq_w] = output[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            pad[:, :, -cutoff_freq_h:, -cutoff_freq_w:] = output[:, :, -cutoff_freq_h:, -cutoff_freq_w:]\t\n",
    "\n",
    "    return pad\n",
    "\n",
    "def DiscreteHartleyTransform(input):\n",
    "    fft = torch.rfft(input, 2, normalized=True, onesided=False)\n",
    "    # for new version of pytorch\n",
    "    #fft = torch.fft.fft2(input, dim=(-2, -1), norm='ortho')\n",
    "    #fft = torch.stack((fft.real, fft.imag), -1)\n",
    "    dht = fft[:, :, :, :, -2] - fft[:, :, :, :, -1]\n",
    "    return dht\n",
    "\n",
    "class SpectralPoolingFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, oheight, owidth):\n",
    "        ctx.oh = oheight\n",
    "        ctx.ow = owidth\n",
    "        ctx.save_for_backward(input)\n",
    "\n",
    "        # Hartley transform by RFFT\n",
    "        dht = DiscreteHartleyTransform(input)\n",
    "\n",
    "        # frequency cropping\n",
    "        all_together = _spectral_crop(dht, oheight, owidth)\n",
    "        # inverse Hartley transform\n",
    "        dht = DiscreteHartleyTransform(all_together)\n",
    "        return dht\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_variables\n",
    "\n",
    "        # Hartley transform by RFFT\n",
    "        dht = DiscreteHartleyTransform(grad_output)\n",
    "        # frequency padding\n",
    "        grad_input = _spectral_pad(input, dht, ctx.oh, ctx.ow)\n",
    "        # inverse Hartley transform\n",
    "        grad_input = DiscreteHartleyTransform(grad_input)\n",
    "        return grad_input, None, None\n",
    "\n",
    "class SpectralPool2d(nn.Module):\n",
    "    def __init__(self, t_size):\n",
    "        super(SpectralPool2d, self).__init__()\n",
    "        self.t_size = t_size\n",
    "    def forward(self, input):\n",
    "        H, W = input.size(-2), input.size(-1)\n",
    "        #h, w = math.ceil(H*self.scale_factor[0]), math.ceil(W*self.scale_factor[1])\n",
    "        return SpectralPoolingFunction.apply(input, H, self.t_size)\n",
    "\n",
    "\n",
    "\n",
    "class SpectralPooling_layer(nn.Module):\n",
    "    def __init__(self, t_size):\n",
    "        super(SpectralPooling_layer, self).__init__()\n",
    "        self.t_size = t_size\n",
    "        self.SpecPool2d = SpectralPool2d(t_size=t_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input: batch, in_channel, length \n",
    "        x = x.unsqueeze(1)   # input: batch, 1, in_channel, length \n",
    "        out = self.SpecPool2d(x)\n",
    "        return out.squeeze()\n",
    "    \n",
    "    \n",
    "class SymmetricLightweightConv(nn.Module):\n",
    "    def __init__(self, d_size, groups=2, kernel_size=3, padding=1, n_heads=1,\n",
    "                 weight_softmax=True, bias=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = d_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_heads = n_heads\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.weight_softmax = weight_softmax\n",
    "        self.weight = nn.Parameter(torch.Tensor(np.int(n_heads*self.input_size/2), 1, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.input_size)) if bias else None\n",
    "        self.dropout = dropout\n",
    "        self.weight_dropout = 0.1\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, C, T = input.size()\n",
    "        half1 = input[:, :C//2, :]\n",
    "        half2 = input[:, C//2:, :]\n",
    "        half2_flipped = torch.flip(half2, dims=[2])\n",
    "        concatenated_tensor = torch.cat((half1, half2_flipped), dim=1)\n",
    "        reshaped_tensor = concatenated_tensor.view(B * 2, C // 2, T)\n",
    "        #print(reshaped_tensor.shape)\n",
    "        H = self.n_heads\n",
    "        # weight: n_heads, 1, kernel_size \n",
    "        weight = F.softmax(self.weight, dim=-1) if self.weight_softmax else self.weight\n",
    "        weight = F.dropout(weight, self.weight_dropout, training=self.training)\n",
    "        #print(weight.shape)\n",
    "        output = F.conv1d(reshaped_tensor, weight, padding=self.padding, groups=np.int(self.groups/2))\n",
    "        #print(output.shape)\n",
    "        output = output.view(B, C, T)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.view(1, -1, 1)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class Conv1DEncoder(nn.Module):\n",
    "    def __init__(self, d_size):\n",
    "        super(Conv1DEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # input: batch, in_channel, length \n",
    "            # conv1d: in_channel, out_channel, kernel, stride, padding\n",
    "            # size: (in_size-kernel+2*padding)/stride + 1\n",
    "            # l_out: batch, out_channel, length\n",
    "\n",
    "            nn.Conv1d(d_size, 32, 5, 1, 2),  # out: batch * 32 * 112\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2),  # batch * 32 * 56\n",
    "\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(32, 64, 3, 1, 1),  # out: batch * 64 * 56\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(64, 128, 3, 1, 1),  # out: batch * 64 * 56\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x:  batch * C * T\n",
    "        y = self.conv1(x)  # batch * 64 * 112\n",
    "        #print(y.shape)\n",
    "        y = self.conv2(y)\n",
    "        #y = self.conv3(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "class Attention1D(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(Attention1D, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.weight = nn.Linear(in_channel,1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        M = self.tanh(H)  # (batch, seq_len, rnn_size)\n",
    "        alpha = self.weight(M).squeeze(2)  # (batch, seq_len)\n",
    "        alpha = self.softmax(alpha)  # (batch, seq_len)\n",
    "\n",
    "        r = H * alpha.unsqueeze(2) # (batch, seq_len, rnn_size)\n",
    "        r = r.sum(dim=1)  # (batch, rnn_size)\n",
    "\n",
    "        return r, alpha \n",
    "\n",
    "    \n",
    "class Attentional_LSTM_Pool(nn.Module):\n",
    "    def __init__(self, d_size):\n",
    "        super(Attentional_LSTM_Pool,self).__init__()\n",
    "        \n",
    "        self.attention = Attention1D(in_channel=64)\n",
    "        self.lstm = nn.LSTM(input_size=d_size, hidden_size=32, num_layers=3, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # x:  B * T * C\n",
    "        x1, (ht,ct) = self.lstm(x) # x1: B, T, bi*hidden_size\n",
    "        x1, _ = self.attention(x1) # out: batch, bi*hidden_size: 64\n",
    "        x2 = torch.max(x, 1, keepdim=False)[0] #  B * C\n",
    "        out = torch.cat((x1,x2),dim=1)        \n",
    "        return out, x1\n",
    "\n",
    "\n",
    "class Conv1DLSTM_All(nn.Module):\n",
    "    def __init__(self, d_size, t_size, num_classes=9):\n",
    "        super(Conv1DLSTM_All, self).__init__()\n",
    "        self.d_size = d_size\n",
    "        self.t_size = t_size\n",
    "        self.conv0 = SymmetricLightweightConv(d_size=self.d_size,groups=self.d_size, kernel_size=3, padding=1, n_heads=1)\n",
    "        self.pool = SpectralPooling_layer(self.t_size)\n",
    "        self.conv1 = Conv1DEncoder(d_size = self.d_size)\n",
    "        self.attention_lstm_pool = Attentional_LSTM_Pool(d_size = 64)\n",
    "        self.fc = nn.Linear(64+64, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x: batch * 1 * 224 * 224\n",
    "        y = x.squeeze(1)  # batch * C * T\n",
    "        y = self.conv0(y)\n",
    "        y = self.pool(y) # batch * C * T\n",
    "        y = self.conv1(y)  # batch * C * T\n",
    "        y = y.transpose(2, 1)  # batch * T(64) * C(128)\n",
    "        out, f_st = self.attention_lstm_pool(y)  # out: batch * 112 * 64\n",
    "        out = self.fc(out)\n",
    "        print(f_st.shape, y.shape)\n",
    "        \n",
    "        return out, f_st, y\n",
    "    \n",
    "\n",
    "def smoothSeq(seq):\n",
    "    cumulative_sum = torch.cumsum(seq, dim=1)\n",
    "    accumulated_time = torch.arange(1, seq.size(1) + 1, dtype=seq.dtype, device=seq.device)\n",
    "    smoothed_seq = cumulative_sum / accumulated_time.view(1, seq.size(1), 1)\n",
    "    return smoothed_seq\n",
    "\n",
    "\n",
    "def softplus(x):\n",
    "    return torch.log(1 + torch.exp(x))\n",
    "\n",
    "\n",
    "def rank_loss(f_st, f, beta):\n",
    "    loss = 0\n",
    "    _, length, feature_size = f.shape\n",
    "    f_smooth = smoothSeq(f)\n",
    "    for i in range(length-1):\n",
    "        theta = torch.sum(f_st.squeeze() * f_smooth[:, i+1, :].squeeze(), dim=1) - torch.sum(f_st.squeeze() * f_smooth[:, i, :].squeeze(), dim=1) + beta\n",
    "        time_loss = softplus(theta) \n",
    "        #print(time_loss)\n",
    "        #print(loss)\n",
    "        loss += time_loss\n",
    "    loss /=  length-1\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Attention1D(nn.Module):\n",
    "    def __init__(self, in_channel:int):\n",
    "        super(Attention1D, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.weight = nn.Linear(in_channel,1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        M = self.tanh(H)  # (batch, seq_len, rnn_size)  seq_len可以理解为时间维 rnn_size为lstm输入\n",
    "        alpha = self.weight(M).squeeze(2)  # (batch, seq_len)\n",
    "        alpha = self.softmax(alpha)  # (batch, seq_len)\n",
    "\n",
    "        r = H * alpha.unsqueeze(2) # (batch, seq_len, rnn_size)\n",
    "        r = r.sum(dim=1)  # (batch, rnn_size)\n",
    "\n",
    "        return r, alpha\n",
    "\n",
    "    \n",
    "class Conv1DLSTM(nn.Module):\n",
    "    def __init__(self, num_classes=9, lstm_type='plain'):\n",
    "        super(Conv1DLSTM, self).__init__()\n",
    "        self.lstm_type = lstm_type\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # input: batch, in_channel, length \n",
    "            # conv1d: in_channel, out_channel, kernel, stride, padding\n",
    "            # size: (in_size-kernel+2*padding)/stride + 1\n",
    "            # l_out: batch, out_channel, length\n",
    "            nn.Conv1d(224, 32, 4, 2, 1),  # out: batch * 32 * 112\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2, 2),  # batch * 32 * 56\n",
    "            nn.Conv1d(32, 64, 3, 1, 1),  # out: batch * 64 * 56\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(3, 1, 1),  # batch * 64 * 56\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=32, num_layers=3, batch_first=True, bidirectional=True)\n",
    "        self.attention = Attention1D(in_channel=64)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        self.fc1 = nn.Linear(384, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x: batch * 1 * 224 * 224\n",
    "        y = x.squeeze(1)  # batch * 224 * 224\n",
    "        y = self.conv1(y)  # batch * 64 * 112\n",
    "        y = y.transpose(2, 1)  # batch * 112 * 64\n",
    "        #lstm in: batch, length, feature_in\n",
    "        #lstm out: batch, length, feature_out * 2(bi)\n",
    "   \n",
    "        if self.lstm_type == 'plain':\n",
    "            out, hidden = self.lstm(y)  # out: batch * 112 * 64\n",
    "            f_st = out[:, -1, :] # out: batch * 64\n",
    "            out = self.fc(f_st)\n",
    "            \n",
    "        elif self.lstm_type == 'attention':\n",
    "            out, hidden = self.lstm(y)  # out: batch * 112 * 64\n",
    "            _, alpha = self.attention(out) # out: batch * 64\n",
    "            out = out * alpha.unsqueeze(2)\n",
    "            #print(out.shape)\n",
    "            f_st = out.sum(dim=1)  # (batch, rnn_size)\n",
    "            out = self.fc(f_st)\n",
    "        \n",
    "        return out, f_st, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64]) torch.Size([16, 64, 64])\n",
      "torch.Size([16, 64]) torch.Size([16, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 224\n",
    "channel_d = 224\n",
    "model = Conv1DLSTM_All(d_size=channel_d, t_size=128, num_classes=9)\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BHW\n",
    "out, f_st, y  =  model(f_dt)\n",
    "print(f_st.shape, y.shape)\n",
    "rank_loss(f_st, y, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64]) torch.Size([16, 56, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.7426, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 224\n",
    "channel_d = 224\n",
    "model = Conv1DLSTM(num_classes=9, lstm_type='attention')\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BHW\n",
    "out, f_st, y  =  model(f_dt)\n",
    "print(f_st.shape, y.shape)\n",
    "rank_loss(f_st, y, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pooling import Pooling_layer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SymmetricLightweightConv(nn.Module):\n",
    "    def __init__(self, d_size, groups=2, kernel_size=3, padding=1, n_heads=1,\n",
    "                 weight_softmax=True, bias=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = d_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_heads = n_heads\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.weight_softmax = weight_softmax\n",
    "        self.weight = nn.Parameter(torch.Tensor(np.int(n_heads*self.input_size/2), 1, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.input_size)) if bias else None\n",
    "        self.dropout = dropout\n",
    "        self.weight_dropout = 0.1\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        B, C, T = input.size()\n",
    "        half1 = input[:, :C//2, :]\n",
    "        half2 = input[:, C//2:, :]\n",
    "        half2_flipped = torch.flip(half2, dims=[2])\n",
    "        concatenated_tensor = torch.cat((half1, half2_flipped), dim=1)\n",
    "        reshaped_tensor = concatenated_tensor.view(B * 2, C // 2, T)\n",
    "        #print(reshaped_tensor.shape)\n",
    "        H = self.n_heads\n",
    "        # weight: n_heads, 1, kernel_size \n",
    "        weight = F.softmax(self.weight, dim=-1) if self.weight_softmax else self.weight\n",
    "        weight = F.dropout(weight, self.weight_dropout, training=self.training)\n",
    "        #print(weight.shape)\n",
    "        output = F.conv1d(reshaped_tensor, weight, padding=self.padding, groups=np.int(self.groups/2))\n",
    "        #print(output.shape)\n",
    "        output = output.view(B, C, T)\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias.view(1, -1, 1)\n",
    "        return output\n",
    "\n",
    "    \n",
    "class Conv1DEncoder(nn.Module):\n",
    "    def __init__(self, d_size):\n",
    "        super(Conv1DEncoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # input: batch, in_channel, length \n",
    "            # conv1d: in_channel, out_channel, kernel, stride, padding\n",
    "            # size: (in_size-kernel+2*padding)/stride + 1\n",
    "            # l_out: batch, out_channel, length\n",
    "\n",
    "            nn.Conv1d(d_size, 32, 5, 1, 2),  # out: batch * 32 * 112\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(2),  # batch * 32 * 56\n",
    "\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(32, 64, 3, 1, 1),  # out: batch * 64 * 56\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(64, 128, 3, 1, 1),  # out: batch * 64 * 56\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x:  batch * C * T\n",
    "        y = self.conv1(x)  # batch * 64 * 112\n",
    "        #print(y.shape)\n",
    "        y = self.conv2(y)\n",
    "        y = self.conv3(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "class Attention1D(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(Attention1D, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.weight = nn.Linear(in_channel,1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, H):\n",
    "        M = self.tanh(H)  # (batch, seq_len, rnn_size)\n",
    "        alpha = self.weight(M).squeeze(2)  # (batch, seq_len)\n",
    "        alpha = self.softmax(alpha)  # (batch, seq_len)\n",
    "\n",
    "        r = H * alpha.unsqueeze(2) # (batch, seq_len, rnn_size)\n",
    "        r = r.sum(dim=1)  # (batch, rnn_size)\n",
    "\n",
    "        return r, alpha \n",
    "\n",
    "    \n",
    "class Attentional_LSTM_Pool(nn.Module):\n",
    "    def __init__(self, d_size):\n",
    "        super(Attentional_LSTM_Pool,self).__init__()\n",
    "        \n",
    "        self.attention = Attention1D(in_channel=32)\n",
    "        self.lstm = nn.LSTM(input_size=d_size, hidden_size=32, num_layers=3, batch_first=True, bidirectional=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        # x:  B * T * C\n",
    "        x1, (ht,ct) = self.lstm(x) # x1: B, T, bi*hidden_size\n",
    "        x1, _ = self.attention(x1) # out: batch, bi*hidden_size: 64\n",
    "        x2 = torch.max(x, 1, keepdim=False)[0] #  B * C\n",
    "        out = torch.cat((x1,x2),dim=1)        \n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv1DLSTM_All(nn.Module):\n",
    "    def __init__(self, d_size, t_size, num_classes=9):\n",
    "        super(Conv1DLSTM_All, self).__init__()\n",
    "        self.d_size = d_size\n",
    "        self.t_size = t_size\n",
    "        self.conv0 = SymmetricLightweightConv(d_size=self.d_size,groups=self.d_size, kernel_size=3, padding=1, n_heads=1)\n",
    "        self.pool = SpectralPooling_layer(self.t_size)\n",
    "        self.conv1 = Conv1DEncoder(d_size = self.d_size)\n",
    "        self.attention_lstm_pool = Attentional_LSTM_Pool(d_size = 128)\n",
    "        self.fc = nn.Linear(160, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input x: batch * 1 * 224 * 224\n",
    "        y = x.squeeze(1)  # batch * C * T\n",
    "        y = self.conv0(y)\n",
    "        y = self.pool(y)\n",
    "        y = self.conv1(y)  # batch * 64 * 112\n",
    "        y = y.transpose(2, 1)  # batch * 112 * 64\n",
    "        out = self.attention_lstm_pool(y)  # out: batch * 112 * 64\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 9])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "seq_len_t = 224\n",
    "channel_d = 224\n",
    "model = Conv1DLSTM_All(d_size=channel_d, t_size=128, num_classes=9)\n",
    "f_dt = torch.rand(batch_size, channel_d, seq_len_t) # BHW\n",
    "out =  model(f_dt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import math\n",
    "from torch.nn.modules.utils import _pair\n",
    "\n",
    "def _spectral_crop(input, oheight, owidth):\n",
    "    cutoff_freq_h = math.ceil(oheight / 2)\n",
    "    cutoff_freq_w = math.ceil(owidth / 2)\n",
    "\n",
    "    if oheight % 2 == 1:\n",
    "        if owidth % 2 == 1:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            bottom_left = input[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            bottom_left = input[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:]\n",
    "    else:\n",
    "        if owidth % 2 == 1:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            bottom_left = input[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            top_left = input[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            top_right = input[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            bottom_left = input[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            bottom_right = input[:, :, -cutoff_freq_h:, -cutoff_freq_w:]\n",
    "\n",
    "    top_combined = torch.cat((top_left, top_right), dim=-1)\n",
    "    bottom_combined = torch.cat((bottom_left, bottom_right), dim=-1)\n",
    "    all_together = torch.cat((top_combined, bottom_combined), dim=-2)\n",
    "\n",
    "    return all_together\n",
    "\n",
    "def _spectral_pad(input, output, owidth):\n",
    "    cutoff_freq_w = math.ceil(owidth / 2)\n",
    "    pad = torch.zeros_like(input)\n",
    "\n",
    "    if oheight % 2 == 1:\n",
    "        if owidth % 2 == 1:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):] = output[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w] = output[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):] = output[:, :, -(cutoff_freq_h-1):, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -cutoff_freq_w:] = output[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w] = output[:, :, -(cutoff_freq_h-1):, :cutoff_freq_w]\n",
    "            pad[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:] = output[:, :, -(cutoff_freq_h-1):, -cutoff_freq_w:]\n",
    "    else:\n",
    "        if owidth % 2 == 1:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):] = output[:, :, :cutoff_freq_h, -(cutoff_freq_w-1):]\n",
    "            pad[:, :, -cutoff_freq_h:, :cutoff_freq_w] = output[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            pad[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):] = output[:, :, -cutoff_freq_h:, -(cutoff_freq_w-1):]\n",
    "        else:\n",
    "            pad[:, :, :cutoff_freq_h, :cutoff_freq_w] = output[:, :, :cutoff_freq_h, :cutoff_freq_w]\n",
    "            pad[:, :, :cutoff_freq_h, -cutoff_freq_w:] = output[:, :, :cutoff_freq_h, -cutoff_freq_w:]\n",
    "            pad[:, :, -cutoff_freq_h:, :cutoff_freq_w] = output[:, :, -cutoff_freq_h:, :cutoff_freq_w]\n",
    "            pad[:, :, -cutoff_freq_h:, -cutoff_freq_w:] = output[:, :, -cutoff_freq_h:, -cutoff_freq_w:]\t\n",
    "\n",
    "    return pad\n",
    "\n",
    "def DiscreteHartleyTransform(input):\n",
    "    fft = torch.rfft(input, 2, normalized=True, onesided=False)\n",
    "    # for new version of pytorch\n",
    "    #fft = torch.fft.fft2(input, dim=(-2, -1), norm='ortho')\n",
    "    #fft = torch.stack((fft.real, fft.imag), -1)\n",
    "    dht = fft[:, :, :, :, -2] - fft[:, :, :, :, -1]\n",
    "    return dht\n",
    "\n",
    "class SpectralPoolingFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, owidth):\n",
    "        ctx.ow = owidth\n",
    "        ctx.save_for_backward(input)\n",
    "        # Hartley transform by RFFT\n",
    "        dht = DiscreteHartleyTransform(input)\n",
    "        # frequency cropping\n",
    "        all_together = _spectral_crop(dht, owidth)\n",
    "        # inverse Hartley transform\n",
    "        dht = DiscreteHartleyTransform(all_together)\n",
    "        return dht\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_variables\n",
    "\n",
    "        # Hartley transform by RFFT\n",
    "        dht = DiscreteHartleyTransform(grad_output)\n",
    "        # frequency padding\n",
    "        grad_input = _spectral_pad(input, dht, ctx.ow)\n",
    "        # inverse Hartley transform\n",
    "        grad_input = DiscreteHartleyTransform(grad_input)\n",
    "        return grad_input, None, None\n",
    "\n",
    "class SpectralPool1d(nn.Module):\n",
    "    def __init__(self, t_size):\n",
    "        super(SpectralPool2d, self).__init__()\n",
    "        self.t_size = t_size\n",
    "    def forward(self, input):\n",
    "        #H, W = input.size(-2), input.size(-1)\n",
    "        #h, w = math.ceil(H*self.scale_factor[0]), math.ceil(W*self.scale_factor[1])\n",
    "        return SpectralPoolingFunction.apply(input, self.t_size)\n",
    "\n",
    "\n",
    "\n",
    "class SpectralPooling_layer(nn.Module):\n",
    "    def __init__(self, t_size):\n",
    "        super(SpectralPooling_layer, self).__init__()\n",
    "        self.t_size = t_size\n",
    "        self.SpecPool2d = SpectralPool2d(t_size=t_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input: batch, in_channel, length \n",
    "        x = x.unsqueeze(1)   # input: batch, 1, in_channel, length \n",
    "        out = self.SpecPool1d(x)\n",
    "        return out.squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
